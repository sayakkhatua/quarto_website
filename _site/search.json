[
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "",
    "section": "",
    "text": "Private credit and Property prices: New insights into this nexus with  Cesar Rodriguez \n\n\nAbstract\n\nThe relationship between the dynamics of property prices and private credit has long been a focal point for economists and policymakers, particularly given its role in financial stability. Understanding this relationship is especially crucial for developing economies, where financial markets are often less mature and more vulnerable to external shocks. This paper examines this relationship using quarterly data from 27 countries spanning 1982 to 2021. Through an instrumental variables approach that addresses endogeneity concerns, we identify three key findings. First, property price growth consistently drives private credit growth across both developed and developing economies, with a one percentage point increase in property prices associated with a 0.35 percentage point rise in private credit growth. Second, conventional macroeconomic factors such as interest rates, inflation, and GDP growth affect credit dynamics differently across development levels. Third, external factors, particularly trade openness and commodity price fluctuations, play an especially significant role in shaping credit dynamics in developing countries. Additionally, our analysis suggests that the property price-credit nexus has evolved, with notable shifts occurring around major economic events. Our results are robust to various sensitivity checks and alternative specifications and methodologies. These findings have important implications for the design of macroprudential policies, especially in developing economies where institutional capacities may differ from more developed markets.\n\nStatus: Under Review at Journal of Real Estate Finance and Economics\nTo not move forward is to fall behind: Evidence of Policy Failure from India\n One-page Summary \n\n\nAbstract\n\nThis paper studies the effect of non-implementation of education reforms on enrollment levels at the elementary school level in India. The paper exploits a unique natural experiment in which the Right of Children to Free and Compulsory Education (RTE) Act of 2009 was implemented in India, except for the state (now UT) of Jammu & Kashmir (J&K). Using a quasi-experimental synthetic control approach, I estimate the causal impact of the non-implementation of the RTE act on enrollment levels in elementary schools in J&K. Using household surveys and administrative data from the Ministry of Education, I find that on average nearly 300,000 (16.8% more than pre-RTE enrollments) additional elementary-aged children could have enrolled annually if the act had been rolled out in J&K.\n\nStatus: Draft coming soon\nThe Reliability of Replications: A Study in Computational Reproductions  with Nate Breznau et. al.\n Preprint \n\n\nAbstract\n\nThis paper reports findings from a crowdsourced replication. Eighty-five independent teams attempted a computational replication of results reported in an original study of policy preferences and immigration by fitting the same statistical models to the same data. The replication involved an experimental condition. Random assignment put participating teams into either the transparent group that received the original study and code, or the opaque group receiving only a methods section, rough results description and no code. The transparent group mostly verified the numerical results of the original study with the same sign and p-value threshold (95.7%), while the opaque group had less success (89.3%). Exact numerical reproductions to the second decimal place were far less common (76.9% and 48.1%), and the number of teams who verified at least 95% of all effects in all models they ran was 79.5% and 65.2% respectively. Therefore, the reliability we quantify depends on how reliability is defined, but most definitions suggest it would take a minimum of three independent replications to achieve reliability. Qualitative investigation of the teams’ workflows reveals many causes of error including mistakes and procedural variations. Although minor error across researchers is not surprising, we show this occurs where it is least expected in the case of computational reproduction. Even when we curate the results to boost ecological validity, the error remains large enough to undermine reliability between researchers to some extent. The presence of inter-researcher variability may explain some of the current “reliability crisis” in the social sciences because it may be undetected in all forms of research involving data analysis. The obvious implication of our study is more transparency. Broader implications are that researcher variability adds an additional meta-source of error that may not derive from conscious measurement or modeling decisions, and that replications cannot alone resolve this type of uncertainty.\n\nStatus: Accepted Royal Society Open Science"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "",
    "section": "",
    "text": "Power to the Researchers: Calculating Power After Estimation (Review of Development Economics )  with Alex Tian, Robert Reed, Tom Coupe, Ben Wood\n Working Paper   Published Article \n\n\nAbstract\n\nCalculating statistical power before estimation is considered good practice. However, there is no generally accepted method for calculating power after estimation. There are several reasons why one would want to do this. First, there is general interest in knowing whether ex ante power calculations are dependable guides of actual power. Further, knowing the statistical power of an estimated equation can aid one in interpreting the associated estimates. This study proposes a simple method for calculating power after estimation. To assess its performance, we conduct Monte Carlo experiments customized to produce simulated datasets that resemble actual data from studies funded by the International Initiative for Impact Evaluation (3ie). In addition to the final reports, 3ie provided ex ante power calculations from the funding applications, along with data and code to reproduce the estimates in the final reports. After determining that our method performs adequately, we apply it to the 3ie-funded studies. We find an average ex post power of 75.4%, not far from the 80% commonly claimed in the 3ie funding applications. However, we observe significantly more estimates of low power than would be expected given the ex ante claims. We conclude by providing three examples to illustrate how ex post power can aid the interpretation of estimates that are (i) insignificant and low powered, (ii) insignificant and high powered, and (iii) significant and low powered.\n\nResearch Transparency and Reproducibility at the International Initiative for Impact Evaluation (Journal of Development Effectiveness)  with Sean Grant\n Published Article \n\n\nAbstract\n\nResearch transparency and reproducibility can improve the credibility of scientific evidence on development effectiveness and the utility of this evidence for decision-making. As a funder and producer of research on development effectiveness, the International Initiative for Impact Evaluation (3ie) has several policies and programs that aim to improve research transparency and reproducibility. This manuscript provides a descriptive overview of the history-to-date of research transparency and reproducibility policies and programs at 3ie. In 2012, 3ie launched its Replication Program to incentivize replication of impact evaluations in international development. In 2014, 3ie created the Registry for International Development Impact Evaluations to provide infrastructure for the prospective registration of impact evaluations of development interventions. In 2018, 3ie published its first Research Transparency Policy articulating requirements on the use of open science practices in research activities. In 2022, 3ie created the Transparent, Reproducible, and Ethical Evidence (TREE) Review Framework, which integrates best practices for research transparency and reproducibility into evaluation workflows. This manuscript provides stakeholders in development effectiveness specifically—as well as research grant managers and other organizations in the scientific ecosystem more generally—a descriptive example of institutional efforts to continuously improve research transparency and reproducibility policies and programs.\n\n\n\n\nUsing big data for evaluating development outcomes: a systematic map (Campbell Systematic Reviews)  with F.Rathinam, Z.Siddiqui, M.Malik, P.Duggal, S.Watson, X.Vollenweider\n Published Article \n\n\nAbstract\n\nBackground: Policy makers need access to reliable data to monitor and evaluate the progress of development outcomes and targets such as sustainable development outcomes (SDGs). However, significant data and evidence gaps remain. Lack of resources, limited capacity within governments and logistical difficulties in collecting data are some of the reasons for the data gaps. Big data—that is digitally generated, passively produced and automatically collected—offers a great potential for answering some of the data needs. Satellite and sensors, mobile phone call detail records, online transactions and search data, and social media are some of the examples of big data. Integrating big data with the traditional household surveys and administrative data can complement data availability, quality, granularity, accuracy and frequency, and help measure development outcomes temporally and spatially in a number of new ways.The study maps different sources of big data onto development outcomes (based on SDGs) to identify current evidence base, use and the gaps. The map provides a visual overview of existing and ongoing studies. This study also discusses the risks, biases and ethical challenges in using big data for measuring and evaluating development outcomes. The study is a valuable resource for evaluators, researchers, funders, policymakers and practitioners in their effort to contributing to evidence informed policy making and in achieving the SDGs.\nObjectives: Identify and appraise rigorous impact evaluations (IEs), systematic reviews and the studies that have innovatively used big data to measure any development outcomes with special reference to difficult contexts\nSearch Methods: A number of general and specialised data bases and reporsitories of organisations were searched using keywords related to big data by an information specialist.\nSelection Criteria: The studies were selected on basis of whether they used big data sources to measure or evaluate development outcomes.\nData Collection and Analysis: Data collection was conducted using a data extraction tool and all extracted data was entered into excel and then analysed using Stata. The data analysis involved looking at trends and descriptive statistics only.\nMain Results: The search yielded over 17,000 records, which we then screened down to 437 studies which became the foundation of our systematic map. We found that overall, there is a sizable and rapidly growing number of measurement studies using big data but a much smaller number of IEs. We also see that the bulk of the big data sources are machine-generated (mostly satellites) represented in the light blue. We find that satellite data was used in over 70% of the measurement studies and in over 80% of the IEs.\nAuthors’ Conclusions: This map gives us a sense that there is a lot of work being done to develop appropriate measures using big data which could subsequently be used in IEs. Information on costs, ethics, transparency is lacking in the studies and more work is needed in this area to understand the efficacies related to the use of big data. There are a number of outcomes which are not being studied using big data, either due to the lack to applicability such as education or due to lack of awareness about the new methods and data sources. The map points to a number of gaps as well as opportunities where future researchers can conduct research."
  },
  {
    "objectID": "publications/index.html#book-chapters",
    "href": "publications/index.html#book-chapters",
    "title": "",
    "section": "Book Chapters",
    "text": "Book Chapters\n\n2019\nShort-Term Versus Long-Term Effects of Forced Displacement (in Land Acquisition in Asia)  with Vengadeshvaran Sarma\n Link Here \n\n\nAbstract\n\nThis study focuses on the conceptual frameworks and empirical evidence that underscore forced displacement. In particular, the study explores development-induced displacement and summarises evidence of its short-term and long-terms effects from around the developing world. Evidence in the literature points out to adverse short-term effects among displacees that normalise over the long run. In the short term, adverse psychological, income and cultural factors affect individual and family security and tend to make displacees worse off compared to non-displaced households. In the long term, however, adaptability among displacees and state mechanisms may help displacees normalise and settle down especially if adequate compensation policies are sanctioned."
  },
  {
    "objectID": "publications/index.html#blogs-opinion-pieces",
    "href": "publications/index.html#blogs-opinion-pieces",
    "title": "Publication",
    "section": "Blogs & Opinion Pieces",
    "text": "Blogs & Opinion Pieces"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Sayak Khatua",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sayak Khatua",
    "section": "",
    "text": "Previously, I worked as an Evaluation Specialist for the International Initiative for Impact Evaluation (3ie), on transparency, reproducibility and ethical evidence (TREE) and managed international development projects across Asia and Africa. I also co-founded the Data innovations group at 3ie, which works on pushing the frontiers of evaluation research by using innovative methodologies and data."
  },
  {
    "objectID": "teaching/index.html#university-of-illinois-at-chicago",
    "href": "teaching/index.html#university-of-illinois-at-chicago",
    "title": "Teaching",
    "section": "University of Illinois at Chicago",
    "text": "University of Illinois at Chicago"
  },
  {
    "objectID": "teaching/index.html#portland-state-university",
    "href": "teaching/index.html#portland-state-university",
    "title": "Teaching",
    "section": "Portland State University",
    "text": "Portland State University"
  },
  {
    "objectID": "teaching/index.html#training",
    "href": "teaching/index.html#training",
    "title": "Teaching",
    "section": "Training",
    "text": "Training"
  },
  {
    "objectID": "research/index.html#book-chapters",
    "href": "research/index.html#book-chapters",
    "title": "Research",
    "section": "Book Chapters",
    "text": "Book Chapters\n\n2019\nShort-Term Versus Long-Term Effects of Forced Displacement (in Land Acquisition in Asia) \nwith Vengadeshvaran Sarma\n Link Here \n\n\nAbstract\n\nThis study focuses on the conceptual frameworks and empirical evidence that underscore forced displacement. In particular, the study explores development-induced displacement and summarises evidence of its short-term and long-terms effects from around the developing world. Evidence in the literature points out to adverse short-term effects among displacees that normalise over the long run. In the short term, adverse psychological, income and cultural factors affect individual and family security and tend to make displacees worse off compared to non-displaced households. In the long term, however, adaptability among displacees and state mechanisms may help displacees normalise and settle down especially if adequate compensation policies are sanctioned."
  },
  {
    "objectID": "research/index.html#blogs-opinion-pieces",
    "href": "research/index.html#blogs-opinion-pieces",
    "title": "Research",
    "section": "Blogs & Opinion Pieces",
    "text": "Blogs & Opinion Pieces"
  },
  {
    "objectID": "research/index.html#work-in-progress",
    "href": "research/index.html#work-in-progress",
    "title": "",
    "section": "Work-in-progress",
    "text": "Work-in-progress\nImproving school management in a low-income country: Experimental evidence from India  with Todd Pugatch , Ketki Sheth, Emmanuel Rukundo\n\n\nAbstract\n\n… Coming soon\n\nStatus: Data collection\nWhere morning feels different: Labor market consequences of state reorganisation in India\n\n\nAbstract\n\n… Coming soon\n\nStatus: Data Analysis\nConnecting Communities, Reducing Conflict: Evidence from India’s Rural Road Program\n\n\nAbstract\n\n… Coming soon\n\nStatus: Data Analysis\nWhen Waters Recede: Impact of Extreme Weather Events on Child Labor\n\n\nAbstract\n\n… Coming soon\n\nStatus: Data cleaning\nLearning or Earning: How Mandatory Education Shapes Child Labor in India\n\n\nAbstract\n\n… Coming soon\n\nStatus: Data cleaning\nThe Great (research) Divide: The long term dynamics of Coauthorship Networks in International Development\n\n\nAbstract\n\n… Coming soon\n\nStatus: Gathering data"
  },
  {
    "objectID": "research/index.html#publications",
    "href": "research/index.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n\n2024\nPower to the Researchers: Calculating Power After Estimation (Review of Development Economics )  with Alex Tian, Robert Reed, Tom Coupe, Ben Wood\n Working Paper   Published Article \n\n\n\nAbstract\n\nCalculating statistical power before estimation is considered good practice. However, there is no generally accepted method for calculating power after estimation. There are several reasons why one would want to do this. First, there is general interest in knowing whether ex ante power calculations are dependable guides of actual power. Further, knowing the statistical power of an estimated equation can aid one in interpreting the associated estimates. This study proposes a simple method for calculating power after estimation. To assess its performance, we conduct Monte Carlo experiments customized to produce simulated datasets that resemble actual data from studies funded by the International Initiative for Impact Evaluation (3ie). In addition to the final reports, 3ie provided ex ante power calculations from the funding applications, along with data and code to reproduce the estimates in the final reports. After determining that our method performs adequately, we apply it to the 3ie-funded studies. We find an average ex post power of 75.4%, not far from the 80% commonly claimed in the 3ie funding applications. However, we observe significantly more estimates of low power than would be expected given the ex ante claims. We conclude by providing three examples to illustrate how ex post power can aid the interpretation of estimates that are (i) insignificant and low powered, (ii) insignificant and high powered, and (iii) significant and low powered.\n\nResearch Transparency and Reproducibility at the International Initiative for Impact Evaluation (Journal of Development Effectiveness)  with Sean Grant\n Open Access \n\n\nAbstract\n\nResearch transparency and reproducibility can improve the credibility of scientific evidence on development effectiveness and the utility of this evidence for decision-making. As a funder and producer of research on development effectiveness, the International Initiative for Impact Evaluation (3ie) has several policies and programs that aim to improve research transparency and reproducibility. This manuscript provides a descriptive overview of the history-to-date of research transparency and reproducibility policies and programs at 3ie. In 2012, 3ie launched its Replication Program to incentivize replication of impact evaluations in international development. In 2014, 3ie created the Registry for International Development Impact Evaluations to provide infrastructure for the prospective registration of impact evaluations of development interventions. In 2018, 3ie published its first Research Transparency Policy articulating requirements on the use of open science practices in research activities. In 2022, 3ie created the Transparent, Reproducible, and Ethical Evidence (TREE) Review Framework, which integrates best practices for research transparency and reproducibility into evaluation workflows. This manuscript provides stakeholders in development effectiveness specifically—as well as research grant managers and other organizations in the scientific ecosystem more generally—a descriptive example of institutional efforts to continuously improve research transparency and reproducibility policies and programs.\n\n\n\n2021\nUsing big data for evaluating development outcomes: a systematic map (Campbell Systematic Reviews)  with F.Rathinam, Z.Siddiqui, M.Malik, P.Duggal, S.Watson, X.Vollenweider\n Open Access \n\n\nAbstract\n\nBackground: Policy makers need access to reliable data to monitor and evaluate the progress of development outcomes and targets such as sustainable development outcomes (SDGs). However, significant data and evidence gaps remain. Lack of resources, limited capacity within governments and logistical difficulties in collecting data are some of the reasons for the data gaps. Big data—that is digitally generated, passively produced and automatically collected—offers a great potential for answering some of the data needs. Satellite and sensors, mobile phone call detail records, online transactions and search data, and social media are some of the examples of big data. Integrating big data with the traditional household surveys and administrative data can complement data availability, quality, granularity, accuracy and frequency, and help measure development outcomes temporally and spatially in a number of new ways.The study maps different sources of big data onto development outcomes (based on SDGs) to identify current evidence base, use and the gaps. The map provides a visual overview of existing and ongoing studies. This study also discusses the risks, biases and ethical challenges in using big data for measuring and evaluating development outcomes. The study is a valuable resource for evaluators, researchers, funders, policymakers and practitioners in their effort to contributing to evidence informed policy making and in achieving the SDGs.\nObjectives: Identify and appraise rigorous impact evaluations (IEs), systematic reviews and the studies that have innovatively used big data to measure any development outcomes with special reference to difficult contexts\nSearch Methods: A number of general and specialised data bases and reporsitories of organisations were searched using keywords related to big data by an information specialist.\nSelection Criteria: The studies were selected on basis of whether they used big data sources to measure or evaluate development outcomes.\nData Collection and Analysis: Data collection was conducted using a data extraction tool and all extracted data was entered into excel and then analysed using Stata. The data analysis involved looking at trends and descriptive statistics only.\nMain Results: The search yielded over 17,000 records, which we then screened down to 437 studies which became the foundation of our systematic map. We found that overall, there is a sizable and rapidly growing number of measurement studies using big data but a much smaller number of IEs. We also see that the bulk of the big data sources are machine-generated (mostly satellites) represented in the light blue. We find that satellite data was used in over 70% of the measurement studies and in over 80% of the IEs.\nAuthors’ Conclusions: This map gives us a sense that there is a lot of work being done to develop appropriate measures using big data which could subsequently be used in IEs. Information on costs, ethics, transparency is lacking in the studies and more work is needed in this area to understand the efficacies related to the use of big data. There are a number of outcomes which are not being studied using big data, either due to the lack to applicability such as education or due to lack of awareness about the new methods and data sources. The map points to a number of gaps as well as opportunities where future researchers can conduct research."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "",
    "section": "",
    "text": "Sustainable Living - Practice and Policy (Winter 2023, Fall 2024, Spring 2025 )  Politics of Developing Nations (Summer 2023, Winter 2024, Spring 2024, Winter 2025)  Environmental Politics and Policy (Fall 2023)"
  },
  {
    "objectID": "teaching/index.html#university-of-illinois-at-chicago-teaching-assistant",
    "href": "teaching/index.html#university-of-illinois-at-chicago-teaching-assistant",
    "title": "",
    "section": "University of Illinois at Chicago (Teaching Assistant)",
    "text": "University of Illinois at Chicago (Teaching Assistant)\nIntroduction to Econometrics (Undergraduate)  Principles of Microeconomics (Undergraduate)  Principles of Macroeconomics (Undergraduate)"
  },
  {
    "objectID": "teaching/index.html#portland-state-university-teaching-assistant",
    "href": "teaching/index.html#portland-state-university-teaching-assistant",
    "title": "",
    "section": "Portland State University (Teaching Assistant)",
    "text": "Portland State University (Teaching Assistant)\nAdvanced Econometrics (Graduate)  Advanced Applied Econometrics (Graduate)  Health Economics (Graduate/undergraduate)  Women in the Economy (Graduate/undergraduate)  Multinational Enterprises (Graduate/undergraduate) Labor Economics and Industrial Relations (Graduate/Undergraduate)  Intermediate Microeconomics (Undergraduate)  Principles of Microeconomics (Undergraduate)  Intermediate Macroeconomics (Undergraduate)  Principles of Macroeconomics (Undergraduate)"
  },
  {
    "objectID": "teaching/index.html#trainings",
    "href": "teaching/index.html#trainings",
    "title": "",
    "section": "Trainings",
    "text": "Trainings\nPush Button Replication (Computational Reproducibility)  Research Transparency and Open Science Practices  Impact Evaluation Methodologies using Stata"
  },
  {
    "objectID": "research/index.html#working-papers",
    "href": "research/index.html#working-papers",
    "title": "Research",
    "section": "Working Papers",
    "text": "Working Papers\nHow Many Replicators Does It Take to Achieve Reliability? Investigating Researcher Variability in a Crowdsourced Replication \nwith Nate Breznau et. al.\n Preprint \n\n\nAbstract\n\nThis paper reports findings from a crowdsourced replication. Eighty-five independent teams attempted a computational replication of results reported in an original study of policy preferences and immigration by fitting the same statistical models to the same data. The replication involved an experimental condition. Random assignment put participating teams into either the transparent group that received the original study and code, or the opaque group receiving only a methods section, rough results description and no code. The transparent group mostly verified the numerical results of the original study with the same sign and p-value threshold (95.7%), while the opaque group had less success (89.3%). Exact numerical reproductions to the second decimal place were far less common (76.9% and 48.1%), and the number of teams who verified at least 95% of all effects in all models they ran was 79.5% and 65.2% respectively. Therefore, the reliability we quantify depends on how reliability is defined, but most definitions suggest it would take a minimum of three independent replications to achieve reliability. Qualitative investigation of the teams’ workflows reveals many causes of error including mistakes and procedural variations. Although minor error across researchers is not surprising, we show this occurs where it is least expected in the case of computational reproduction. Even when we curate the results to boost ecological validity, the error remains large enough to undermine reliability between researchers to some extent. The presence of inter-researcher variability may explain some of the current “reliability crisis” in the social sciences because it may be undetected in all forms of research involving data analysis. The obvious implication of our study is more transparency. Broader implications are that researcher variability adds an additional meta-source of error that may not derive from conscious measurement or modeling decisions, and that replications cannot alone resolve this type of uncertainty.\n\nThe Use of Behavioural-science Informed Interventions to Promote Latrine Use \\in Rural India: A Synthesis of Findings \nwith Charlotte Lane and Bethany Caruso\n Preprint   Data and Code \n\nMaking data accessible: lessons learned from computational reproducibility\\ of impact evaluations \nwith Neeta Goel and Marie Gaarder\n Preprint   Data and Code"
  },
  {
    "objectID": "research/index.html#dormant-papers",
    "href": "research/index.html#dormant-papers",
    "title": "",
    "section": "Dormant Papers",
    "text": "Dormant Papers\nThe Use of Behavioural-science Informed Interventions to Promote Latrine Use in Rural India: A Synthesis of Findings \nwith Charlotte Lane and Bethany Caruso\n Preprint   Data and Code \n\nMaking data accessible: lessons learned from computational reproducibility of impact evaluations \nwith Neeta Goel and Marie Gaarder\n Preprint   Data and Code"
  },
  {
    "objectID": "teaching/index.html#international-initiative-for-impact-evaluation-trainer",
    "href": "teaching/index.html#international-initiative-for-impact-evaluation-trainer",
    "title": "",
    "section": "International Initiative for Impact Evaluation (Trainer)",
    "text": "International Initiative for Impact Evaluation (Trainer)\nPush Button Replication (Computational Reproducibility)  Research Transparency and Open Science Practices  Impact Evaluation Methodologies using Stata"
  },
  {
    "objectID": "teaching/index.html#oregon-state-university-teaching-assistant",
    "href": "teaching/index.html#oregon-state-university-teaching-assistant",
    "title": "",
    "section": "Oregon State University (Teaching Assistant)",
    "text": "Oregon State University (Teaching Assistant)\nPolitics of Developing Nations (Fall 2022)  Introduction to Econometrics (Spring 2023)  International Economics (Spring 2023)"
  }
]